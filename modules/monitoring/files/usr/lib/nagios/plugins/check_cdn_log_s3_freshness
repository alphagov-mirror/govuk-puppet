#!/usr/bin/env python3
"""
Icinga check for freshness of GOV.UK Fastly CDN logs in S3.

Checks whether there are sufficiently recent files in the
govuk-{env}-fastly-logs S3 bucket, making use of the naming scheme of the logs
in order to avoid listing the entire bucket (which contains many thousands of
objects).

Needs to work with Python 3.4 (because we're stuck on Trusty for now and don't
want to install extra dependencies because this is for monitoring), so no f''
strings for example :(

The S3 object names look like, for example:

govuk_assets/year=2020/month=01/date=30/2020-01-30T22:00:00.000-DXdcYBSfUdJyVvwbPcAn.log.gz

For help on command-line options:
    check_cdn_log_s3_freshness -h

Example usage for testing the script:
    check_cdn_log_s3_freshness -e staging -t 2020-01-31T17:00

Example usage in production:
    check_cdn_log_s3_freshness -e production -l govuk_www -c 1
"""

import argparse
import datetime
import enum
import logging
import sys

import boto3


class IcingaStatus(enum.IntEnum):
    """Exit statuses for Icinga checks."""
    OK = 0
    WARNING = 1
    CRITICAL = 2


def fromisoformat(s):
    """Parse a string YYYY-MM-DDTHH:MM and return a datetime. Assumes UTC.

    This is only meant for passing in a fake time from the command-line when
    testing. Would use datetime.fromisoformat, but we're stuck on Python 3.4.
    """
    return datetime.datetime.strptime(s, '%Y-%m-%dT%H:%M')


def parse_args():
    """Return an argparse.Namespace populated with command-line args."""
    parser = argparse.ArgumentParser()
    parser.add_argument('-e', '--env', default='production',
                        help='Environment to check: integration, staging, production.')
    parser.add_argument('-l', '--log_type', default='govuk_assets',
                        help='Which logs to check: govuk_assets, govuk_www.')
    parser.add_argument('-c', '--critical_age_hours', type=int, default=1,
                        help='If the newest logs are older than this many hours, '
                             'return CRITICAL status.')
    parser.add_argument('-t', '--time', type=fromisoformat,
                        help='For testing purposes, use the given time as if it\'s the current '
                             'time. Requires the format YYYY-MM-DDTHH:MM. Assumes UTC.')
    parser.add_argument('-v', '--verbose', action='count',
                        help='Show DEBUG log messages.')
    return parser.parse_args()


def get_s3_bucket(env):
    """Given the environment name, return the boto S3 resource for the bucket."""
    s3 = boto3.resource('s3')
    bucket_name = 'govuk-%s-fastly-logs' % env
    logging.info('S3 bucket name: %s', bucket_name)
    return s3.Bucket(bucket_name)


def get_prefix(log_type, critical_age_hours, fake_time=None):
    """Return the S3 prefix to be searched for log files, based on the time."""
    max_age = datetime.timedelta(hours=critical_age_hours)
    now = fake_time or datetime.datetime.now(datetime.timezone.utc)  # TZ-aware datetime in UTC.
    query_time = now - max_age  # Oldest acceptable last_modified time for logs.
    time_prefix = query_time.strftime('year=%Y/month=%m/date=%d/%Y-%m-%dT%H')
    logging.info('Time is %s%s', query_time.isoformat(),
                 ' (overridden for testing)' if fake_time else '')
    return '%s/%s' % (log_type, time_prefix)


def logs_new_enough(s3_objs):
    """True if there are new enough logs in the given listing, False otherwise."""
    # For now, we just check that there were logs within the right hour.
    # Could improve precision by parsing the timestamp in the filename and
    # comparing that against the threshold, but that's probably not warranted
    # unless we really care about having better precision that +/- an hour.
    obj_count = sum(1 for _ in s3_objs)
    logging.info('Found %d log chunks.', obj_count)
    return obj_count > 0


def exit_status_for_icinga(s3_objs):
    """Return the appropriate exit status, based on whether the logs are up-to-date."""
    if logs_new_enough(s3_objs):
        return IcingaStatus.OK
    return IcingaStatus.CRITICAL


def main():
    args = parse_args()
    logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)

    bucket = get_s3_bucket(args.env)
    prefix = get_prefix(args.log_type, args.critical_age_hours, fake_time=args.time)
    logging.info('Listing S3 bucket for prefix %s', prefix)
    listing = bucket.objects.filter(Prefix=prefix)

    status = exit_status_for_icinga(listing)
    print(status.name)
    sys.exit(status)

if __name__ == '__main__':
    main()
